---
layout: single
title:  "What I'm reading today (2025-11-13)"
date:   2025-11-13 10:00:00 +0800
classes: wide
toc: true
categories:
- research
- all_posts
permalink: /research/2025-11-13-what-im-reading
---

# What I'm reading today (2025-11-13)

A brief compilation of things I'm reading and looking at today (and across the next couple of days). Recording my readings like this helps keep me accountable to understanding what I'm reading, while also keeping it light and sustainable.

## 1. ChatGPT is not A Man but Das Man: Representativeness and Structural Consistency of Silicon Samples Generated by Large Language Models

**[Paper link](https://www.alphaxiv.org/overview/2507.02919v1)**

This fits into the existing literature of "why don't we just have AI agents simulate real humans in surveys/psychology experiments?". Like other work, it flags a downside of assuming that AI agents = humans.

Let's say you're conducting a survey and you need to ask a "representative sample" of people. The first thing you try in trying to get a representative sample of people is to, assuming that you can't go survey people yourself, to just ask an LLM to imagine that it were a white person, or a black person, or a rich person, or a poor person and trying to ground the LLM in a particular persona. You can imagine doing this at a large scale, for example, at Tencent, they did this to [create 1 billion artificial personas](https://arxiv.org/abs/2406.20094).

### AI personas not being faithful human simulations should make sense

What this paper and other papers in this space argue is that even if you were to create a billion synthetic AI personas, if you ask them to have authentic interactions and give their opinions on particular questions, answer certain questions, and things of the like, the responses are not nearly as diverse as humans.

Having a particular set of characteristics should not necessarily mean that the person should adopt a particular opinion, especially in politics. As an example, it is known that people of particular ethnicities or socio-economic status may have a direction that they vote in, but are not a monolith. And so one thing is that if you use AI agents for say simulation work or political opinions you would imagine a similar distribution where someone being of a particular category does not necessarily mean that they have a one-to-one opinion. But in practice, as this paper and other papers have shown, you tend to delve towards stereotypes and opinion homgeneity. However, this is in conflict with the training objective of LLMs, especially in the pre-training phase, which the authors here coin as the "accuracy optimization hypothesis". It's also a generally known tension when training a model that is supposed to converge to the next token prediction task.

Another consideration goes beyond the training task and it goes towards stereotyping more broadly, where you have a lack of information about the totality of a person's life and you reduce the calculation of their beliefs based on a set of labels. This should make sense as a necessarily incomplete knowledge task. You don't have enough information to define the space of outcomes with super high accuracy. After all, not all people are the same within any group (e.g., not all Republicans are the same, not all Democrats are the same, not all ages are the same, not all rich people are the same, not all able-bodied people are the same). But this sort of task asks models to make predictions about responses and opinions based solely on a surpassed level set of traits.

But let's say even if it were possible to know and to have complete information on all of the events that compose someone's life, such that you get much more rich detail beyond just a person's political stance or opinions about particular social topics. I still argue, as this paper does as well, that even that would not be sufficient. I think that there are two compelling reasons to do so:

1. The LLM doesn't know which of the particular facts or facets about someone's opinion take precedence in which case and why, and frankly in general people don't either. People are very random and noisy and stochastic, and they also can't a hundred percent explain why they have their beliefs.
2. It's also the idea that LLMs suffer from optimizing for accuracy of the masses. Plenty of papers have shown this result, and this is a common tension now between LLMs that are good in general vs LLMs that are personalized. Perspectives and opinions that are in the minority classes or ever that minority class is have a pressure to be pushed out because of the next word prediction task, which the authors here coin as the accuracy optimization hypothesis. 

Like with all the work in this sort of simulation space though, I think that the devil is in the details. It seems like different papers operationalize it, which is why some argue that it is possible to do this sort of simulation and some argue that it's not. I think it's also a consideration of the measures that are being used and the kind of use cases that are being considered. There are particular use cases that I think are more prone to this opinion collapse, and I am interested in looking to see when this happens more. But some initial things that would come to mind to test are:

1. The representation and distribution of that opinion in the training corpus. If you have an opinion that is overrepresented in the corpus (such as a common culture or opinion), I imagine that would be more prone to a dominance by a majority opinion because it would have a stronger effect on the weight updates than an opinion that is very sparse and rare.
2. Two other concepts or characteristics that would correlate with that opinion. This may be trying to delve into whatever sense of morality or a theory of mind that models might have. But perhaps there are other characteristics that correlate well with a particular opinion that might take precedence over having an exact opinion that was in the training corpus.  I can imagine something like the opinion about a movie where it's a newer movie that may not be in the training corpus, but you might describe what the movie is like or who's in it. There are other factors that might be activated in the neurons of the LLM that could be interesting to probe at from an interpretability perspective to show what actually does go on if you say, "Oh, what do you think about this new movie by Matt Damon?" (assuming that the LLM can't use web search, of course).

All that to say, an interesting paper that fits into a wider literature that still has a lot to be worked out.
