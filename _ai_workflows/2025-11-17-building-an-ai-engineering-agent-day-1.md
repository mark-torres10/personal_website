---
layout: single
title: "Building an AI Engineering Agent, Day 1: Bootstrapping on Titanic"
date: 2025-11-17 12:00:00 +0800
classes: wide
toc: true
categories:
- ai_workflows
- all_posts
permalink: /ai_workflows/2025-11-17-building-an-ai-engineering-agent-day-1
---

# Building an AI Engineering Agent, Day 1: Bootstrapping on Titanic

I'm working on building an AI agent that can automate data science and ML engineering tasks. Not only is this an unsolved problem (though lots of people are working on it), it's also a great way to learn how to build good, useful agents.

I'm starting with the Titanic dataset. I chose this dataset because:

- It's pretty simple. It's one of the first datasets you work with when learning data science.
- The actual solution is pretty well-known: absent of some feature engineering, the task is pretty simple.

I wanted to get a very bare-bones v0 that I can continue to iterate on this week.

## Creating a simple one-shot prompt for doing this

I started with a really basic one-shot prompt to do this:

```python
system_prompt = """
You are an expert ML engineer.
Given the Titanic Kaggle train and test DataFrames already loaded as `train` and `test`,
generate executable Python code that:

1. Performs simple preprocessing (e.g., fill NA, encode categoricals).
2. Trains a model to predict 'Survived'.
3. Produces predictions for the test set.
4. Returns a DataFrame with columns ['PassengerId', 'Survived'] in a variable named `submission`.
5. Do not import the CSVs againâ€”they are already loaded. Just operate on `train` and `test`.
"""
```

I had a very basic pipeline for doing this:

```python
import anthropic

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

# Call the Opus model
message_opus = client.messages.create(
    model="claude-opus-4-1-20250805",
    max_tokens=1500,
    system=system_prompt,
    messages=[
        {"role": "user", "content": user_prompt}
    ]
)
```

I ran into a variety of errors when doing this really naive approach, namely:

- The code wasn't guaranteed to compile. In fact, it often didn't compile. I had a hard time trying to do the parsing correctly to get it to work.
- I needed to set up a sandbox environment for the code to run

## Adding code helper tooling and a sandbox environment

### Getting the code to compile

### Adding a sandbox environment

## Optimizing the prompt

## Adding a basic Streamlit UI

## Abstracting out the feature engineering from the ML training

## Checking the updated results in the Streamlit UI

We can take a look below at how the results are currently looking from the Streamlit UI.

First, let's take a look at the current generated prompt being used.

![Streamlit: picture of the LLM query](/assets/images/2025-11-17-building-an-ai-engineering-agent-day-1/1.png)

Then let's take a look at the function code generated by the model.

![Streamlit: picture of the generated code](/assets/images/2025-11-17-building-an-ai-engineering-agent-day-1/2.png)

Lastly, let's take a look at some results after training a model on the features engineered by the model.

![Streamlit: picture of the training results on the test set](/assets/images/2025-11-17-building-an-ai-engineering-agent-day-1/3.png)

## Things I want to check out next
